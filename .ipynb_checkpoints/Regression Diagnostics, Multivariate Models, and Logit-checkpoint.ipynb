{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named Quandl",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f8674d3b5082>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mQuandl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msmf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named Quandl"
     ]
    }
   ],
   "source": [
    "# Load relevant libraries.\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import Quandl\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "import patsy\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook', style='whitegrid', palette='deep', font='sans-serif', font_scale=1, rc=None)\n",
    "import urllib2 as url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naapl_all = Quandl.get(\"YAHOO/AAPL\", trim_start=\"2010-1-1\", trim_end=\"2015-09-18\")\\nnasdaq_all = Quandl.get(\"NASDAQOMX/COMP\", trim_start=\"2010-1-1\", trim_end=\"2015-09-18\")\\naapl = aapl_all[\\'Adjusted Close\\']\\nnasdaq = nasdaq_all[\\'Index Value\\']\\naapl_returns = np.log(aapl / aapl.shift(1))\\nnasdaq_returns = np.log(nasdaq / nasdaq.shift(1))\\naapl_returns = aapl_returns.dropna()\\nnasdaq_returns = nasdaq_returns.dropna()\\naapl_returns = pd.DataFrame(aapl_returns)\\nnasdaq_returns = pd.DataFrame(nasdaq_returns)\\ndata = pd.merge(nasdaq_returns, aapl_returns, left_index=True, right_index=True)\\ndata.rename(columns={\\'Index Value\\':\\'nasdaq\\', \\'Adjusted Close\\':\\'aapl\\'}, inplace=True)\\nmod = smf.ols(formula=\\'aapl ~ nasdaq\\', data = data).fit()\\nprint(mod.summary())\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One more time.\n",
    "'''\n",
    "aapl_all = Quandl.get(\"YAHOO/AAPL\", trim_start=\"2010-1-1\", trim_end=\"2015-09-18\")\n",
    "nasdaq_all = Quandl.get(\"NASDAQOMX/COMP\", trim_start=\"2010-1-1\", trim_end=\"2015-09-18\")\n",
    "aapl = aapl_all['Adjusted Close']\n",
    "nasdaq = nasdaq_all['Index Value']\n",
    "aapl_returns = np.log(aapl / aapl.shift(1))\n",
    "nasdaq_returns = np.log(nasdaq / nasdaq.shift(1))\n",
    "aapl_returns = aapl_returns.dropna()\n",
    "nasdaq_returns = nasdaq_returns.dropna()\n",
    "aapl_returns = pd.DataFrame(aapl_returns)\n",
    "nasdaq_returns = pd.DataFrame(nasdaq_returns)\n",
    "data = pd.merge(nasdaq_returns, aapl_returns, left_index=True, right_index=True)\n",
    "data.rename(columns={'Index Value':'nasdaq', 'Adjusted Close':'aapl'}, inplace=True)\n",
    "mod = smf.ols(formula='aapl ~ nasdaq', data = data).fit()\n",
    "print(mod.summary())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R$^2$\n",
    "\n",
    "We already know that different packages produce different regression output, which are typically called \"regression diagnosgics.\"  Python's statsmodels basically mirrors the regression diagnostics produced by Stata.  We have already reviewed coefficients, standard errors, t values, and confidence intervals. \n",
    "\n",
    "As I noted, the R$^2$ \"goodness of fit\" metric is a frequently-cited regression diagnostic.  If a linear regression uses a constant (which should be included in practice), the R$^2$ is bounded between 0 and 1.  It measures the share of the variation in $y$ explained by the variation in the features used in a model.  Given this definition, \"bigger is better\" is the first place that people go to evaluate the quality of the model, which is unwarranted.  \n",
    "\n",
    "\"However, it can still be challenging to determine what is a good R$^2$ value, and in general, this will depend on the application.  For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error.  In this case, we would expect to see an R$^2$ value that is extremely close to 1, and a substantially smaller R$^2$ might indicate serious problems with the experiment in which the data were generated.  On the other hand, in typical application in biology, pyschology, marketing and other domains, the linear model is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large.  In this setting, we would expect only a very small proportion of the variacne in the response to be explained by the predictor, and an R$^2$ value well below 0.1 might be more realistic.\"  Trevor Hastie, Robert Tibshirani, et al.\n",
    "\n",
    "The example I showed last time was meant to highlight this very point.  \n",
    "\n",
    "## Other Diagnostics to Assess Model Quality\n",
    "\n",
    "Adjusted R$^2$: A metric that captures the penalty in the use of a large number of features with little explanatory power.  \n",
    "\n",
    "AIC (the Akaike Information Criterion): a measure of the relative quality of a statistical model for a given set of data.  It captures the trade-off between the goodness of fit and the complexity of the model.  For example, if I have $N$ $(y,x)$ pairs, I could in principle fit an $N-1$ degree polynomial that passed through all points.  \n",
    "\n",
    "BIC (the Bayes Information Criterion): another measure of relative quality.  Rule of thumb: chose the model with the lowest BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is trivial to extend the single-feature linear model to a linear model that simultaneously incorporates multiple features.  The interpretation of the results of a statistical model that uses multiple features is the same the interpretation of the partial derivative from the calculus of many variables: the effect of a small change in a particular feature on a label (or outcome).  \n",
    "\n",
    "A model with $K$ features, $x_{ik}$ and label $y_i$:\n",
    "\n",
    "$y_i=\\sum_{k=1}^Kx_{ik}\\cdot\\beta_k+\\epsilon_i = x_i^\\prime \\beta + \\epsilon_i$\n",
    "\n",
    "The $K$ features $x_{ik}$ influence the label $y_i$ through the $K$-vector, $\\beta$, which we estimate statistically.  A specific partial derivative interpretation.\n",
    "\n",
    "${\\displaystyle \\frac{\\partial E(y_i)}{\\partial x_{ik}}=\\beta_k}$\n",
    "\n",
    "(For those interested in ancient history: Frisch–Waugh–Lovell theorem.)\n",
    "\n",
    "Bottom line is simple: Fit the linear model with multiple features.  The basic approach to hypothesis testing remains unchanged. The challenge is the interpretation of the results, which we will discuss in detail.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data: Property sales records of single-family, owner-occupied houses in Staten Island between 2003 and early 2013.  \n",
    "# See https://app.enigma.io/table/us.states.ny.cities.nyc.property.annualized-sales?row=0&col=0&page=1&documentView=false\n",
    "# Often times lacking the convenience of an API, \n",
    "# it may be necessary to load data that has been formatted using other statistical learning environments.\n",
    "# Python's pandas allows us to do so in Python.\n",
    "# R's foreign library allow us to do so in R.\n",
    "\n",
    "sales = pd.read_stata('SI Sales.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DATA DICTIONARY\n",
    "# price: Sales price in $\n",
    "# unit_size: Size of dwelling in square meters\n",
    "# land_size: Size of land on which dwelling sits in square meters\n",
    "# age: Age of dwelling in years at sale\n",
    "# todt: Indicator (or dummy variable) of whether a dwelling is in Todt Hill, Staten Island\n",
    "# sales_year: Year of sale\n",
    " \n",
    "print sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "# Note that I have cleaned the data for \"outliers\" (outside of the scope of this class).\n",
    "\n",
    "print sales.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's examine the correlation matrix.\n",
    "\n",
    "print sales.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start with bivariate linear model that explains the variation in sales prices by the size of the dwelling.\n",
    "\n",
    "mod = smf.ols(formula='price ~ unit_size', data = sales).fit()\n",
    "print(mod.summary())\n",
    "\n",
    "# How to intepret these results?  For every additional square meter of dwelling size, sales price is ~$1,658 higher.\n",
    "# The 95% Confidence Interval of this effect is [1630, 1686].\n",
    "# Average unit size is 161 square meters, with a standard deviation of 79 square meters.  \n",
    "# Therefore, an increase of one standard deviation in dwelling size would increase the sales price by 79 * $1,656 = $130,824."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alternative table layout.\n",
    "\n",
    "mod.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perhaps we believe that more than one feature can simultaneously affect sales prices.\n",
    "# We can easily use multivariate regression to account for many features at once.\n",
    "# When we add an additional feature, namely the land size, we find that it has an independent effect.\n",
    "# Holding constant unit_size, an increase in land_size of one square meter independently increases the sales price by $268.\n",
    "# In addition, note that the effect of unit size on price has fallen from $1,658 to $1,221.\n",
    "# Moreover, the 95% Confidence Intervals do not overlap.\n",
    "# This is evidence of omitted variable bias (sometimes called the \"collinearity problem\").  \n",
    "# Failing to include land_size has biased up the measured effect of unit_size because the two features are positively correlated.\n",
    "# The two features, however, have independent effects on sales prices.\n",
    "# In this example, we can compare the R-squared measures across the two models because one is nested in the other.  \n",
    "# We see that the R-squared rises from 0.302 to 0.387, indicating a considerable improvement in the fit of the data.\n",
    "\n",
    "mod = smf.ols(formula='price ~ unit_size + land_size', data = sales).fit()\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's examine what happens when we account for the age of the dwelling at the date of sale.\n",
    "# Thoughts about how age might affect sales prices?\n",
    "# Negative impact: old houses (of equal size) how lower sales prices.\n",
    "# Note, however, that there is little impact on the effects of the other features when we add age to the model.\n",
    "\n",
    "mod = smf.ols(formula='price ~ unit_size + land_size + age', data = sales).fit()\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is an example of using dummy or indicator variables to capture potential \"discontinuities\" in our model.\n",
    "# I am told that by people at CUSP that Todt Hill is an upscale area in Staten Island.\n",
    "# Suppose we want to capture this feature of the data, which we can do using the indicator \"todt\". \n",
    "\n",
    "mod = smf.ols(formula='price ~ unit_size + land_size + age + todt', data = sales).fit()\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scaling of data.  \n",
    "# Suppose we wanted to rescale the sales prices because they are measured in $ (and are large).\n",
    "# Rescale the label by dividing by 1000.\n",
    "\n",
    "sales['priceper1000'] = sales['price']/1000\n",
    "print sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How does this rescaling affect our estimates?\n",
    "\n",
    "mod = smf.ols(formula='priceper1000 ~ unit_size + land_size + age + todt', data = sales).fit()\n",
    "print(mod.summary())\n",
    "#sm.stats.sandwich_covariance.cov_hc0(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It may be more relevant to consider this in terms of percent changes in values.  \n",
    "# For example, what is the percentage rate of change in sales prices given a percent change in unit size.\n",
    "# In economics, these are elasticities, which are unit-free measures.\n",
    "\n",
    "sales['logprice'] = np.log(sales['price'])\n",
    "sales['logunit'] = np.log(sales['unit_size'])\n",
    "sales['logland'] = np.log(sales['land_size'])\n",
    "sales['logage'] = np.log(sales['age']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A 10% change in unit size increases sales price by 3.4%.\n",
    "# A 10% change in land size increases sales price by 2.6%.\n",
    "# A 10% change in age decreases sales prices by 0.5%.\n",
    "# Prices in Todt Hill are about 44% higher holding all else constant.\n",
    "# Is this model directly comparable to the one above?\n",
    "# No because we non-linearly transformed both the label and the features.\n",
    "\n",
    "mod = smf.ols(formula='logprice ~ logunit + logland + logage + todt', data = sales).fit()\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Linear Probability Model and The Logistic Classifier (or \"Logit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem above examined a continuous label or dependent variable.  That is, $y$ was quantitative: the sales prices of single-family houses in Staten Island.  Often, we wish to examine a discrete or categorical label.  What is a categorical label?  An example would be eye color: $\\{$brown, green, blue$\\}$.  \n",
    "\n",
    "In economics, this is considered to be analysis of \"limited dependent variables,\" while in computer science, it is classification.  This different terminology refers to the same thing.  We will begin with the linear probability model (with multiple attributes), which allows us to seemless transition to the Logistic Classifier (or \"Logit\").\n",
    "\n",
    "This simple classifier is the ground upon which data scientists of all stripes converge.  It is a very important methodological technique that I want you to understand thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can think of categorical variable as being driven by an underlying DGP, and we as observers, observe only outcomes.\n",
    "\n",
    "Consider: $y_i^*=x_i^\\prime\\beta+\\epsilon_i$\n",
    "\n",
    "$y_i^*$ is a latent value for person i that is unobserved by us.  For example, you do not observe the value I place on this bottle of water.  You simple observe me drinking it.  In other words, you observe the outcome my choice set $\\{$drink, not drink$\\}$.  Another example would be observing someone taking a taxi.  You do not observe the value that someone places on the taxi ride.  You know that she has many alternative methods of transportation, all with varying values to her, but you observe the outcome of her choice.\n",
    "\n",
    "$x_i$ are a vector of features (or attributes or predictors or independent variables).  We observe these.\n",
    "\n",
    "$\\beta$ is a vector that measures how features affect the latent index, which we will estimate statistically. \n",
    "\n",
    "$\\epsilon_i$ retains its status as our ignorance.  \n",
    "\n",
    "What do we observe?\n",
    "\n",
    "${\\displaystyle d_i = }$\n",
    "$\\left\\{ \\begin{array}{l l} \n",
    "{1} & \\quad \\text{if person i takes a cab, which happens when } y_i^*\\ge0\\\\ \n",
    "{0} & \\quad \\text{if person i does not take a cab, which happens when } y_i^*\\lt0 \\\\\n",
    "\\end{array} \\right.$\n",
    "\n",
    "The latent variable approach has become a very popular modeling tool in statistical learning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in some data on graduate school admissions.  These data are fictional, but useful.\n",
    "\n",
    "data = pd.read_csv(\"http://www.ats.ucla.edu/stat/data/binary.csv\")\n",
    "print data.info()\n",
    "print data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Session\n",
    "\n",
    "Find the correlation matrix between the variables.\n",
    "Run a few linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# School rank is a categorical feature, and we should capture this aspect using C(rank)\n",
    "# C(rank) tells statsmodels to convert the categorical variable into indicator variables and omit one of them\n",
    "# The omitted categorical indicator is the reference category.\n",
    "# Let's interpret the results from the linear probability model using the summary statistics from the data.\n",
    "# GRE: Graduate Record Examine: a one point increase in GRE increases probability of admission by 0.0004 or 0.04%\n",
    "# GPA: Grade Point Average: a one point increase in GPA increases probability of admission by 0.156 or 15.6%.  \n",
    "# rank: Going to a tier 2 school reduces probabilty of admission by 0.1624 or 16.24% (relative to a tier 1 school).\n",
    "\n",
    "mod = smf.ols(formula='admit ~ gre + gpa + C(rank)', data = data).fit()\n",
    "print(mod.summary())\n",
    "#print data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's make a prediction of admission for the \"average\" applicant at a tier 2 school.\n",
    "# How might we generate a 95% confidence interval on this prediction?  You did it in Homework 2.  It's called bootstrapping.\n",
    "# Remember the prediction for later.\n",
    "\n",
    "print data.describe()\n",
    "print \n",
    "print mod.params\n",
    "print\n",
    "print (mod.params['Intercept'] + mod.params['C(rank)[T.2]'] + mod.params['gre'] *587.7 + mod.params['gpa'] * 3.3899)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logistic Classifier\n",
    "\n",
    "We are going to change the linear model to the following:\n",
    "\n",
    "$Pr(d_i=1)=x_i^\\prime\\beta+\\epsilon_i$\n",
    "\n",
    "We will then impose a distributional assumption on $\\epsilon_i$, namely that it is logistically distributed.\n",
    "\n",
    "${\\displaystyle Pr(d_i=1) = \\frac{\\exp(x_i^\\prime\\beta)}{1+\\exp(x_i^\\prime\\beta)}}$ \n",
    "\n",
    "We see immediately that this is NOT a linear model (that is, a model that is linear in $\\beta$).\n",
    "\n",
    "${\\displaystyle Pr(d_i=0) = 1 - Pr(d_i=1) = 1 - \\frac{\\exp(x_i^\\prime\\beta)}{1+\\exp(x_i^\\prime\\beta)} = \\frac{1}{1+\\exp(x_i^\\prime\\beta)}}$\n",
    "\n",
    "An \"odds ratio\":\n",
    "\n",
    "${\\displaystyle \\frac{Pr(d_i=1)}{Pr(d_i=0)} = \\frac{Pr(d_i=1)}{1 - Pr(d_i=1)} = \\exp(x_i^\\prime\\beta)}$\n",
    "\n",
    "This implies that the log-odds ratio (or \"logit\") is:\n",
    "\n",
    "${\\displaystyle \\log\\big(\\frac{Pr(d_i=1)}{1 - Pr(d_i=1)}\\big) = x_i^\\prime\\beta}$, which is linear in $\\beta$.\n",
    "\n",
    "To address the estimation of the parameters of interest, we need to construct a likelihood function that we will tell the computer to optimize.  Start by constructing the likelihood for observation $i$:\n",
    "\n",
    "${\\displaystyle l_i = Pr(d_i=1)^{d_i}\\cdot Pr(d_i=0)^{(1-d_i)}=\\frac{\\exp(x_i^\\prime\\beta)}{1+\\exp(x_i^\\prime\\beta)}^{d_1}\\frac{1}{1+\\exp(x_i^\\prime\\beta)}^{(1-d_i)}}$\n",
    "\n",
    "If we make some assumptions we can write:\n",
    "\n",
    "${\\displaystyle L = \\prod_{i=1}^N l_i = \\prod_{i=1}^N \\frac{\\exp(x_i^\\prime\\beta)}{1+\\exp(x_i^\\prime\\beta)}^{d_1}\\frac{1}{1+\\exp(x_i^\\prime\\beta)}^{(1-d_i)}}$\n",
    "\n",
    "Goal is to tell the computer to maximize $L$ with respect to $\\beta$ given the data we have. \n",
    "\n",
    "Once we have done that, we can make probabilistic predictions (or classifications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's fit the logit.\n",
    "\n",
    "logit_mod = smf.logit('admit ~ gre + gpa + C(rank)', data = data).fit()\n",
    "print\n",
    "print(logit_mod.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these logit coefficients?\n",
    "Big lesson: Unlike the linear probability model, shown below, they cannot be directly interpreted.\n",
    "Note, however, the similarities.  \n",
    "\n",
    "1. The signs are identical.\n",
    "2. The relative magnitudes are similar.\n",
    "3. The z and the t are (relatively) similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Again we can make a prediction, the probability of admission or Pr(d=1), \n",
    "# using the logistic formula above.\n",
    "# Average features at rank 2 school.\n",
    "\n",
    "print data.describe()\n",
    "print \n",
    "print logit_mod.params\n",
    "print\n",
    "e = np.exp(logit_mod.params['Intercept'] + logit_mod.params['C(rank)[T.2]'] + \n",
    "           logit_mod.params['gre'] *587.7 + logit_mod.params['gpa'] * 3.3899)\n",
    "print(e / (1 + e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also generate marginal effects, \n",
    "# which are the change in probability of admission given a \"small\" change in the features. \n",
    "# Note their similarilities to the regression estimates from the linear probability model.\n",
    "\n",
    "marginal = logit_mod.get_margeff()\n",
    "print(marginal.summary())\n",
    "print\n",
    "print mod.params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
