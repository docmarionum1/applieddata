{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Linear Model: The Returns to Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load relevant libraries.\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "import patsy\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook', style='whitegrid', palette='deep', font='sans-serif', font_scale=1, rc=None)\n",
    "import urllib2 as url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Multivariate Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is trivial to extend the single-feature linear model to a linear model that simultaneously incorporates multiple features.  The interpretation of the results of a statistical model that uses multiple features is the same the interpretation of the partial derivative from the calculus of many variables: the effect of a small change in a particular feature on a label (or outcome).  \n",
    "\n",
    "A model with $K$ features, $x_{ik}$ and label $y_i$:\n",
    "\n",
    "$y_i=\\sum_{k=1}^Kx_{ik}\\cdot\\beta_k+\\epsilon_i = x_i^\\prime \\beta + \\epsilon_i$\n",
    "\n",
    "The $K$ features $x_{ik}$ influence the label $y_i$ through the $K$-vector, $\\beta$, which we estimate statistically.  A specific partial derivative interpretation.\n",
    "\n",
    "${\\displaystyle \\frac{\\partial E(y_i)}{\\partial x_{ik}}=\\beta_k}$\n",
    "\n",
    "(For those interested in ancient history: Frisch–Waugh–Lovell theorem.)\n",
    "\n",
    "Bottom line is simple: Fit the linear model with multiple features.  The basic approach to hypothesis testing remains unchanged. The challenge is the interpretation of the results, which we will discuss in detail.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another application: Why does someone go to school \n",
    "# (and should it be publicly subsidized)?\n",
    "# Load Griliches data\n",
    "# Paper is here: http://web.stanford.edu/~pista/griliches71.pdf\n",
    "# Bio is here: https://en.wikipedia.org/wiki/Zvi_Griliches\n",
    "\n",
    "griliches = pd.read_csv('griliches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DATA DICTIONARY\n",
    "# Here: http://rpackages.ianhowson.com/cran/SciencesPo/man/griliches76.html\n",
    " \n",
    "print griliches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "\n",
    "print griliches.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a dataframe, easy to plot.\n",
    "\n",
    "plt.figure()\n",
    "griliches['lw'].hist(bins = 50)\n",
    "plt.title(\"Histogram of Log Wages\", size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "griliches['s'].hist(bins = 50)\n",
    "plt.title(\"Histogram of Education\", size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start with bivariate linear model.\n",
    "\n",
    "mod = smf.ols(formula='lw ~ s', data = griliches).fit()\n",
    "print(mod.summary())\n",
    "\n",
    "# Every additional year of education increases wages by about 10%.\n",
    "# 95% Confidence Interval is [0.085, 0.108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We don't need to stay in this limited world.\n",
    "# Coding help: Generating indicator variables using Pandas.\n",
    "\n",
    "print pd.get_dummies(griliches['s'] == 16).head(20)\n",
    "griliches['undergrad'] = pd.get_dummies(griliches['s'] == 16)[1]\n",
    "print\n",
    "print griliches['undergrad'].head(20)\n",
    "\n",
    "mod = smf.ols(formula='lw ~ rns + mrt + smsa + med + iq + age + tenure + s', data = griliches).fit()\n",
    "print\n",
    "print(mod.summary())\n",
    "\n",
    "# Notice that taking into account \"ability\" through an IQ score cuts in half the measured return to education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Linear Probability Model and The Logistic Classifier (or \"Logit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem above examined a continuous label or dependent variable.  That is, $y$ was quantitative: the sales prices of single-family houses in Staten Island.  Often, we wish to examine a discrete or categorical label.  What is a categorical label?  An example would be eye color: $\\{$brown, green, blue$\\}$.  \n",
    "\n",
    "In economics, this is considered to be analysis of \"limited dependent variables,\" while in computer science, it is classification.  This different terminology refers to the same thing.  We will begin with the linear probability model (with multiple attributes), which allows us to seemless transition to the Logistic Classifier (or \"Logit\").\n",
    "\n",
    "This simple classifier is the ground upon which data scientists of all stripes converge.  It is a very important methodological technique that I want you to understand thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can think of categorical variable as being driven by an underlying DGP, and we as observers, observe only outcomes.\n",
    "\n",
    "Consider: $y_i^*=x_i^\\prime\\beta+\\epsilon_i$\n",
    "\n",
    "$y_i^*$ is a latent value for person i that is unobserved by us.  For example, you do not observe the value I place on this bottle of water.  You simple observe me drinking it.  In other words, you observe the outcome my choice set $\\{$drink, not drink$\\}$.  Another example would be observing someone taking a taxi.  You do not observe the value that someone places on the taxi ride.  You know that she has many alternative methods of transportation, all with varying values to her, but you observe the outcome of her choice.\n",
    "\n",
    "$x_i$ are a vector of features (or attributes or predictors or independent variables).  We observe these.\n",
    "\n",
    "$\\beta$ is a vector that measures how features affect the latent index, which we will estimate statistically. \n",
    "\n",
    "$\\epsilon_i$ retains its status as our ignorance.  \n",
    "\n",
    "What do we observe?\n",
    "\n",
    "${\\displaystyle d_i = }$\n",
    "$\\left\\{ \\begin{array}{l l} \n",
    "{1} & \\quad \\text{if person i takes a cab, which happens when } y_i^*\\ge0\\\\ \n",
    "{0} & \\quad \\text{if person i does not take a cab, which happens when } y_i^*\\lt0 \\\\\n",
    "\\end{array} \\right.$\n",
    "\n",
    "The latent variable approach has become a very popular modeling tool in statistical learning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in some data on graduate school admissions.  These data are fictional, but useful.\n",
    "\n",
    "data = pd.read_csv(\"http://www.ats.ucla.edu/stat/data/binary.csv\")\n",
    "print data.info()\n",
    "print data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# School rank is a categorical feature, and we should capture this aspect using C(rank)\n",
    "# C(rank) tells statsmodels to convert the categorical variable into indicator variables \n",
    "# and omit one of them\n",
    "# The omitted categorical indicator is the reference category.\n",
    "# Let's interpret the results from the linear probability model using the summary statistics \n",
    "# from the data.\n",
    "# GRE: Graduate Record Examine: a one point increase in GRE increases probability of admission \n",
    "# by 0.0004 or 0.04%\n",
    "# GPA: Grade Point Average: a one point increase in GPA increases probability of admission \n",
    "# by 0.156 or 15.6%.  \n",
    "# rank: Going to a tier 2 school reduces probabilty of admission by 0.1624 or 16.24% \n",
    "# (relative to a tier 1 school).\n",
    "# Note new line of code to generate predicted values: the labels that would be predicted using the model's attributes as is.\n",
    "\n",
    "mod = smf.ols(formula='admit ~ gre + gpa + C(rank)', data = data).fit()\n",
    "olshat = mod.predict(data)\n",
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's make a prediction of admission for the \"average\" applicant (over all) coming from a tier 2 school.\n",
    "# How might we generate a 95% confidence interval on this prediction?  You did it in Homework 2.  It's called bootstrapping.\n",
    "# Remember the prediction for later.\n",
    "\n",
    "print data.describe()\n",
    "print \n",
    "print mod.params\n",
    "print\n",
    "print (mod.params['Intercept'] + mod.params['C(rank)[T.2]'] + mod.params['gre'] *587.7 + mod.params['gpa'] * 3.3899)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logistic Classifier\n",
    "\n",
    "Again, we start with a sample of size, $N$.  We are going to change the linear model to the following:\n",
    "\n",
    "$Pr(d_i=1)=x_i^\\prime\\beta+\\epsilon_i$\n",
    "\n",
    "We will then impose a distributional assumption on $\\epsilon_i$, namely that it is logistically distributed.\n",
    "\n",
    "${\\displaystyle Pr(d_i=1) = \\frac{\\exp(x_i^\\prime\\beta)}{1+\\exp(x_i^\\prime\\beta)}}$ \n",
    "\n",
    "We see immediately that this is NOT a linear model (that is, a model that is linear in $\\beta$).\n",
    "\n",
    "${\\displaystyle Pr(d_i=0) = 1 - Pr(d_i=1) = 1 - \\frac{\\exp(x_i^\\prime\\beta)}{1+\\exp(x_i^\\prime\\beta)} = \\frac{1}{1+\\exp(x_i^\\prime\\beta)}}$\n",
    "\n",
    "An \"odds ratio\":\n",
    "\n",
    "${\\displaystyle \\frac{Pr(d_i=1)}{Pr(d_i=0)} = \\frac{Pr(d_i=1)}{1 - Pr(d_i=1)} = \\exp(x_i^\\prime\\beta)}$\n",
    "\n",
    "This implies that the log-odds ratio (or \"logit\") is:\n",
    "\n",
    "${\\displaystyle \\log\\big(\\frac{Pr(d_i=1)}{1 - Pr(d_i=1)}\\big) = x_i^\\prime\\beta}$, which is linear in $\\beta$.\n",
    "\n",
    "To address the estimation of the parameters of interest, we need to construct a likelihood function that we will tell the computer to optimize.  Start by constructing the likelihood for observation $i$:\n",
    "\n",
    "${\\displaystyle l_i = Pr(d_i=1)^{d_i}\\cdot Pr(d_i=0)^{(1-d_i)}=\\frac{\\exp(x_i^\\prime\\beta)}{1+\\exp(x_i^\\prime\\beta)}^{d_1}\\frac{1}{1+\\exp(x_i^\\prime\\beta)}^{(1-d_i)}}$\n",
    "\n",
    "If we make some assumptions we can write the likelihood function for the sample, $L$:\n",
    "\n",
    "${\\displaystyle L = \\prod_{i=1}^N l_i = \\prod_{i=1}^N \\frac{\\exp(x_i^\\prime\\beta)}{1+\\exp(x_i^\\prime\\beta)}^{d_1}\\frac{1}{1+\\exp(x_i^\\prime\\beta)}^{(1-d_i)}}$\n",
    "\n",
    "Goal is to tell the computer to maximize $L$ with respect to $\\beta$ given the data we have. \n",
    "\n",
    "Once we have done that, we can make probabilistic predictions (or classifications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's fit the logit and generate predicted values.\n",
    "\n",
    "logit_mod = smf.logit('admit ~ gre + gpa + C(rank)', data = data).fit()\n",
    "print\n",
    "print(logit_mod.summary())\n",
    "logithat = logit_mod.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these logit coefficients?\n",
    "Big lesson: Unlike the linear probability model, shown below, they cannot be directly interpreted.\n",
    "Note, however, the similarities.  \n",
    "\n",
    "1. The signs are identical.\n",
    "2. The relative magnitudes are similar.\n",
    "3. The z and the t are (relatively) similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Again we can make a prediction, the probability of admission or Pr(d=1), \n",
    "# using the logistic formula above.\n",
    "# Average features at rank 2 school.\n",
    "\n",
    "print data.describe()\n",
    "print \n",
    "print logit_mod.params\n",
    "print\n",
    "e = np.exp(logit_mod.params['Intercept'] + logit_mod.params['C(rank)[T.2]'] + \n",
    "           logit_mod.params['gre'] *587.7 + logit_mod.params['gpa'] * 3.3899)\n",
    "print(e / (1 + e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also generate marginal effects, \n",
    "# which are the change in probability of admission given a \"small\" change in the features. \n",
    "# Note their similarilities to the regression estimates from the linear probability model.\n",
    "\n",
    "marginal = logit_mod.get_margeff()\n",
    "print(marginal.summary())\n",
    "print\n",
    "print mod.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# They hit it on average (as they should).\n",
    "\n",
    "print data['admit'].mean(), olshat.mean(), logithat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(olshat, logithat, 'bo')\n",
    "plt.xlim(-.1, .8)\n",
    "plt.ylim(-.1, .8)\n",
    "plt.xlabel(r'$LPM$', fontsize = 16)\n",
    "plt.ylabel(r'$Logit$', fontsize = 16)\n",
    "plt.title(r'Comparison of Logit and LPM', fontsize = 20)\n",
    "plt.axvline(0.0, color='black', ls='--', lw=2.0)\n",
    "plt.axvline(0.3175, color='g', ls='--', lw=1.0)\n",
    "plt.axhline(0.0, color='black', ls='--', lw=2.0)\n",
    "plt.axhline(0.3175, color='g', ls='--', lw=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violations of Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To talk about violations, we need to state explicitly the assumptions that underlie our approach to date.\n",
    "\n",
    "1. A linear model with $K$ features, $y_i = x_i^\\prime \\beta + \\epsilon_i$ or $y=X\\beta + \\epsilon$.\n",
    "2. $E(\\epsilon) = 0$: errors are on average zero.\n",
    "3. $Var(\\epsilon) = \\sigma^2 I_N$.  (Spherical errors.)\n",
    "4. $Cov(X, \\epsilon) = 0$\n",
    "5. $\\epsilon \\sim N(0, \\sigma^2 I_N)$.  This is a very strong assuption and wholely un-necessary.  People like to assert to because it implies that least squares and maximum likelihood are equivalent.  \n",
    "\n",
    "If 1 through 4 above hold, then least squares is \"BLUE\": the Best Linear Unbiased Estimator.  You cannot do better.  If 5 is added to the mix, least squares is \"MVUE\": the Minimum Variance Unbiased Estimator.  (See Cramer Rao bound: https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What matters here and why?\n",
    "\n",
    "1. One reason we have developed a large number of arrows for our quiver of statistical learning.  One appeal of the linear model is interpretation of partial derivative effects.  If goal is prediction rather than having point estimates, other statistcal learning techniques better if more costly from computational point of view.  (Much less of an issue now, as \"we never turn off the machine.\")\n",
    "2. Not really.  \n",
    "3. Heteroscedasticity (or heteroskedasticity).  If our errors are non-spherical, we are not accurately measuring the standard errors that we need to do hypothesis testing.  Therefore, we may not be able to make valid statistical inferences.  We have developed methods to account for this (see https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors) that can be easily invoked using Statsmodels (Python) or GLS (R).   \n",
    "4. Omitted Variable Bias.  For me, this is a very important issue.  In the homework, you estimated a toy model that suffered from omitted variable bias.  The presence of this bias would affect both hypothesis testing and prediction.  It would also impact other techniques developed in associated with a violation of 1.  \n",
    "5. Not really."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
